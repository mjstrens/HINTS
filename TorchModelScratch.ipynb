{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Load Dataset\n",
    "# Step 2. Make Dataset Iterable\n",
    "# Step 3. Create Model Class\n",
    "# Step 4. Instantiate Model Class\n",
    "# Step 5. Instantiate Loss Class\n",
    "# Step 6. Instantiate Optimizer Class\n",
    "# Step 7. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = dsets.MNIST(root='./data', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "#train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x) # pytorch Crossentropy loss takes scores, not logits as inputs\n",
    "        #outputs = F.softmax(self.linear(x), dim = -1) # why better without F.softmax\n",
    "        return outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# was not worth caching this\n",
    "def get_minibatch(dataset, n, i, input_dim = 784):\n",
    "    lst1, lst2 = zip(*[dataset[ii] for ii in range(i * n, (i+1) * n)])\n",
    "    return(torch.stack(lst1).view(-1, input_dim), torch.stack(lst2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "Iteration: 600. Loss: 53.662323. Accuracy: 90.16.\n",
      "Iteration: 1200. Loss: 36.005524. Accuracy: 90.95.\n",
      "Iteration: 1800. Loss: 33.190990. Accuracy: 91.43.\n",
      "Iteration: 2400. Loss: 31.717945. Accuracy: 91.56.\n",
      "Wall time: 24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_dim = 784\n",
    "output_dim = 10\n",
    "\n",
    "# Note we will use a Bayesian model average of (some distribution over) history of sampled class probs in the sampling case\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction = 'sum') # computes softmax and then the cross entropy\n",
    "model = LogisticRegression(input_dim, output_dim)\n",
    "lr_rate = 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate)\n",
    "N = train_dataset.__len__()//batch_size # TO DO: get N from dataset\n",
    "print(N)\n",
    "epochs = 4 #n_iters / (len(train_dataset) / batch_size)\n",
    "i = 0\n",
    "for epoch in range(int(epochs)):\n",
    "    epoch_losses = []\n",
    "    for it in range(N):\n",
    "        optimizer.zero_grad()\n",
    "        images, labels = get_minibatch(train_dataset, batch_size, it)    \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels) # NEXT STEP: replace with an evaluate call!!!\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.detach().item())\n",
    "        i += 1\n",
    "    # end of epoch: calculate Accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        outputs = model(images)\n",
    "        # TO DO convert these outputs to probs, to get more accurate Accuracy metric\n",
    "        # and to support historical averaging (e.g. q(n) = p(n) * 2/(n+1) + q(n-1) * (n-1)/(n+1))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
    "        correct+= (predicted == labels).sum()\n",
    "    accuracy = 100 * float(correct)/total\n",
    "    print(\"Iteration: {}. Loss: {:1f}. Accuracy: {}.\".format(i, torch.tensor(epoch_losses).mean(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "tensor(-237.9083, grad_fn=<NegBackward>)\n",
      "torch.Size([10, 784]) torch.Size([10, 784])\n",
      "torch.Size([10]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "from HINTS_fn import *\n",
    "# PyTorch test function for HINTS\n",
    "# proposals are Langevin/HMC and must create a new deep copy of the model\n",
    "# HINTS uses the ID() of the model object for tracking/caching purposes\n",
    "\n",
    "class TorchMNIST(UserFn):\n",
    "        def __init__(self, additive = True, state_is_hashable = True):\n",
    "            self.batch_size = 100\n",
    "            self.input_dim = 784\n",
    "            self.output_dim = 10\n",
    "            self.train_dataset = dsets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "            self.N = self.train_dataset.__len__()//self.batch_size # num scenarios\n",
    "            self.lr = 0.001 # TO DO pass this parameter\n",
    "            self.additive = additive # used by HINTS\n",
    "            print(self.N)\n",
    "            self.criterion = torch.nn.CrossEntropyLoss(reduction = 'sum')\n",
    "            super().__init__(None, state_is_hashable)\n",
    "        #\n",
    "        def user_sample_initial_state(self):\n",
    "            model = LogisticRegression(self.input_dim, self.output_dim)\n",
    "            return(model)\n",
    "        #    \n",
    "        # the return value will be cached, but the gradient computed at level zero will\n",
    "        # be present in the state only immediately after a cache miss (first call for each scenario, state)\n",
    "        def user_evaluate(self, state, term_index, level):\n",
    "            if level == 0:\n",
    "                f = -self.minibatch_loss(state, term_index)\n",
    "                f.backward()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    f = -self.minibatch_loss(state, term_index)\n",
    "            return(f)\n",
    "        #\n",
    "        # this can be called with or without torch.no_grad\n",
    "        def minibatch_loss(self, model, term_index):\n",
    "            images, labels = get_minibatch(self.train_dataset, self.batch_size, term_index)  \n",
    "            outputs = model(images)\n",
    "            loss = self.criterion(outputs, labels) # NEXT STEP: replace with an evaluate call!!!\n",
    "            return(loss)\n",
    "\n",
    "    \n",
    "MH = TorchMNIST()\n",
    "state0 = MH.user_sample_initial_state()\n",
    "v = MH.user_evaluate(state0, 7, 0)\n",
    "print(v)\n",
    "for f in state0.parameters():\n",
    "    print(f.shape, f.grad.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Langevin \n",
    "#### this treatment assumes we are trying to minimise ... I should reverse sign of f\n",
    "p ~ N(0,1)\n",
    "p1 = p - eps/2 g(x)\n",
    "x' = x + p1\n",
    "p' = p1 - eps/2 g(x') = p - eps/2 (g(x') + g(x)) = p - eps gbar\n",
    "\n",
    "accept if u < exp(-dH)\n",
    "where dH = f(x') - f(x) + p'^2/2 - p^2/2\n",
    "\n",
    "second term is Hastings correction:\n",
    "p^2 - 2 p eps gbar + eps^2 gbar^2 - p^2 = eps gbar (eps gbar - 2p)\n",
    "\n",
    "in terms of p1: \n",
    "p1^2 - eps p1 g(x') + (eps/2)^2 g(x')^2 - p1^2 - eps p1 g(x) - (eps/2)^2 g(x)^2\n",
    "\n",
    "Alternative would be to include p as part of state (which we'll need to do for HMC anyway)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 600. Loss: 53.543919. Accuracy: 90.27.\n",
      "Iteration: 1200. Loss: 35.941975. Accuracy: 90.97.\n",
      "Iteration: 1800. Loss: 33.138775. Accuracy: 91.41.\n",
      "Iteration: 2400. Loss: 31.673714. Accuracy: 91.63.\n",
      "Wall time: 24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 4\n",
    "lr_rate = 0.001\n",
    "model = MH.user_sample_initial_state()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = lr_rate)\n",
    "i = 0\n",
    "for epoch in range(int(epochs)):\n",
    "    epoch_losses = []\n",
    "    for it in range(MH.N):\n",
    "        optimizer.zero_grad()\n",
    "        loss = MH.minibatch_loss(model, it)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.detach().item())\n",
    "        i += 1\n",
    "    # end of epoch: calculate Accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        outputs = model(images)\n",
    "        # TO DO convert these outputs to probs, to get more accurate Accuracy metric\n",
    "        # and to support historical averaging (e.g. q(n) = p(n) * 2/(n+1) + q(n-1) * (n-1)/(n+1))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
    "        correct+= (predicted == labels).sum()\n",
    "    accuracy = 100 * float(correct)/total\n",
    "    print(\"Iteration: {}. Loss: {:1f}. Accuracy: {}.\".format(i, torch.tensor(epoch_losses).mean(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HINTS import *\n",
    "class HINTS_HMC(HINTS):\n",
    "    # TO DO move langevin out?\n",
    "    def __init__(self, args, fn):\n",
    "        super().__init__(args, fn)\n",
    "        self.epsilon = args.epsilon\n",
    "    #\n",
    "    # HMC version ... correction depends on knowing gradient at proposed state\n",
    "    def primitive_move(self, model, index = 0):\n",
    "        scenarios = self.scenarios(0, index)\n",
    "        #print(\"primitive\", index, len(scenarios))\n",
    "        v = self.fn(model, scenarios, 0) # puts a gradient into state as a side effect for HMC\n",
    "        current = self.fn.user_sample_initial_state() # empty model\n",
    "        # do this with no grad...\n",
    "        correction = 0.0\n",
    "        for f, f_prime in zip(model.parameters(), current.parameters()):\n",
    "            # f.grad.data is the right shape to store momentum temporarily\n",
    "            ###TO DO how big is f.grad.data compared with unit noise\n",
    "            f.grad.data = 1.0 * torch.randn(f.shape) + self.epsilon * 0.5 * f.grad.data # TO DO check gradient convention\n",
    "            f_prime.data = f.data + 0.5 * self.epsilon * f.grad.data # add momentum\n",
    "            correction -= 0.5 * (f.grad.data * f.grad.data).sum() # Kinetic energy term of -H\n",
    "        # compute the value of the new state and its gradient\n",
    "        v_prime = self.fn(current, scenarios, 0)\n",
    "        # store the new momentum in the grad entries of the candidate model\n",
    "        for f, f_prime in zip(model.parameters(), current.parameters()):\n",
    "            p_prime = f.grad.data + self.epsilon * 0.5 * f_prime.grad.data # TO DO check gradient convention\n",
    "            correction += 0.5 * (p_prime * p_prime).sum() # kinetic energy term of H_new\n",
    "            f.grad = None # must not reuse (unless we do more leapfrog steps)\n",
    "            f_prime.grad = None\n",
    "        \n",
    "        # standard MHR / HINTS acceptance\n",
    "        vdiff = (v_prime - v)/self.Ts[0] # PE change ... these are cached evaluations, no side effects\n",
    "        #print(v_prime, v, self.Ts[0], correction)\n",
    "        #\n",
    "        #correction = 0 # TEMPOARY OVERRDE - SGD\n",
    "        #\n",
    "        #\n",
    "        accept = self.metropolis_accept(vdiff - correction)\n",
    "        #if not(accept):\n",
    "        #    print(\"REJECT!\")\n",
    "        (self.acceptances if accept else self.rejections)[0] += 1\n",
    "        return((current, vdiff) if accept else (model, 0.0))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.additive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "{'levels': 6, 'design': array([1, 2, 3, 2, 5, 2, 5]), 'additive': True, 'T': 1.0, 'dT': 0.0, 'epsilon': 0.02}\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_known_args()[0] # defaults\n",
    "\n",
    "# checked regular MCMC is accurate (1 level, lbf = 6)\n",
    "if True:\n",
    "    args.levels = 6\n",
    "    log_branch_factor = 1\n",
    "    N_0 = 1\n",
    "    #args.design = np.array([N_0] + [2 ** log_branch_factor for l in range(args.levels)])\n",
    "    args.design = np.array([1,2,3,2,5,2,5])\n",
    "    NUM_SCENARIOS = args.design.prod()\n",
    "    aa = False\n",
    "    #NUM_SCENARIOS = N_0 * 2 ** (args.levels * log_branch_factor) # TO DO get from HINTS\n",
    "else: # test Langevin MCMC first\n",
    "    args.levels = 1 \n",
    "    NUM_SCENARIOS = 600 # 600 for naive mcmc, 1 for SGD\n",
    "    args.design = np.array([1,NUM_SCENARIOS]) # ensure whole dataset is covered\n",
    "    aa = True\n",
    "    # use always_accept flag to get MCMC\n",
    "    \n",
    "print(NUM_SCENARIOS)\n",
    "\n",
    "# design now has levels + 1 entries\n",
    "\n",
    "# additive log probability is more natural from a Bayesian perspective but both are valid\n",
    "\n",
    "args.additive = True # effectively selects a different temperature structure when False (= average or expectation)\n",
    "\n",
    "args.T = 1.0 #top level\n",
    "args.dT = 0.0 if args.additive else 0.5 # temperature increment by level (mainly for optimisation or averaging structure)\n",
    "args.epsilon = 0.02 # for HMC\n",
    "print(args.__dict__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "RESET\n",
      "6\n",
      "[  1   2   6  12  60 120 600]\n",
      "600\n",
      "[1. 1. 1. 1. 1. 1. 1.]\n",
      "LogisticRegression(\n",
      "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "g = TorchMNIST(args.additive, state_is_hashable = True)\n",
    "hmc = HINTS_HMC(args, g)\n",
    "state  = g.sample_initial_state()\n",
    "print(state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Iteration: 1. Loss: 111.57228088378906. Accuracy: 63.04. [475 237  83  46  10   5   1] [125  63  17   4   0   0   0]\n",
      "1\n",
      "Iteration: 2. Loss: 89.43009185791016. Accuracy: 69.35. [1024  457  148   79   19   10    2] [176 143  52  21   1   0   0]\n",
      "2\n",
      "Iteration: 3. Loss: 97.63916015625. Accuracy: 71.83. [1585  662  208  114   28   15    3] [215 238  92  36   2   0   0]\n",
      "3\n",
      "Iteration: 4. Loss: 57.772796630859375. Accuracy: 74.41. [2135  879  262  147   36   20    4] [265 321 138  53   4   0   0]\n",
      "4\n",
      "Iteration: 5. Loss: 69.00692749023438. Accuracy: 75.93. [2683 1092  312  183   41   25    5] [317 408 188  67   9   0   0]\n",
      "5\n",
      "Iteration: 6. Loss: 90.3050537109375. Accuracy: 76.84. [3259 1316  359  219   47   29    6] [341 484 241  81  13   1   0]\n",
      "6\n",
      "Iteration: 7. Loss: 84.50695037841797. Accuracy: 77.76. [3821 1531  410  255   53   34    7] [379 569 290  95  17   1   0]\n",
      "7\n",
      "Iteration: 8. Loss: 60.63371658325195. Accuracy: 78.35. [4365 1757  461  287   61   39    8] [435 643 339 113  19   1   0]\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "for t in range(1000):\n",
    "    hmc.shuffle()\n",
    "    print(t)\n",
    "    state, correction = hmc.hints(state, args.levels, always_accept = aa) # e.g. dbg = (t==0)\n",
    "    g.cached_evaluate.cache_clear() # risk of revisiting same state and scenario after reject\n",
    "    # diagnostic histogram\n",
    "    # show progress\n",
    "    if True:\n",
    "        with torch.no_grad():\n",
    "            loss = MH.minibatch_loss(state, t % NUM_SCENARIOS)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = Variable(images.view(-1, 28*28))\n",
    "            outputs = state(images)\n",
    "            # TO DO convert these outputs to probs, to get more accurate Accuracy metric\n",
    "            # and to support historical averaging (e.g. q(n) = p(n) * 2/(n+1) + q(n-1) * (n-1)/(n+1))\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
    "            correct+= (predicted == labels).sum()\n",
    "        accuracy = 100 * float(correct)/total\n",
    "        print(\"Iteration: {}. Loss: {}. Accuracy: {}.\"\\\n",
    "              .format(t+1, loss, accuracy), hmc.acceptances, hmc.rejections)\n",
    "\n",
    "#TO DO skip accept/reject at top level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks to be better than MCMC\n",
    "# TO DO higher branch factor (4+) more efficient\n",
    "# aim for bigger moves at primitive level (or HMC chain?)\n",
    "# GPU\n",
    "# Bayesian accuracy measure thru decaying average (triangle distrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
