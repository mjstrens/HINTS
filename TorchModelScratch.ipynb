{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Load Dataset\n",
    "# Step 2. Make Dataset Iterable\n",
    "# Step 3. Create Model Class\n",
    "# Step 4. Instantiate Model Class\n",
    "# Step 5. Instantiate Loss Class\n",
    "# Step 6. Instantiate Optimizer Class\n",
    "# Step 7. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = dsets.MNIST(root='./data', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "#train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x) # pytorch Crossentropy loss takes scores, not logits as inputs\n",
    "        #outputs = F.softmax(self.linear(x), dim = -1) # why better without F.softmax\n",
    "        return outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# was not worth caching this\n",
    "def get_minibatch(dataset, n, i, input_dim = 784):\n",
    "    lst1, lst2 = zip(*[dataset[ii] for ii in range(i * n, (i+1) * n)])\n",
    "    xs = torch.stack(lst1).view(-1, input_dim)\n",
    "    ys = torch.tensor(lst2)\n",
    "    return(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "Iteration: 600. Loss: 53.603058. Accuracy: 90.16.\n",
      "Iteration: 1200. Loss: 35.963299. Accuracy: 90.93.\n",
      "Iteration: 1800. Loss: 33.158787. Accuracy: 91.46.\n",
      "Iteration: 2400. Loss: 31.691315. Accuracy: 91.63.\n",
      "CPU times: user 1min 10s, sys: 2min 47s, total: 3min 58s\n",
      "Wall time: 22.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_dim = 784\n",
    "output_dim = 10\n",
    "\n",
    "# Note we will use a Bayesian model average of (some distribution over) history of sampled class probs in the sampling case\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction = 'sum') # computes softmax and then the cross entropy\n",
    "model = LogisticRegression(input_dim, output_dim)\n",
    "lr_rate = 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate)\n",
    "N = train_dataset.__len__()//batch_size # TO DO: get N from dataset\n",
    "print(N)\n",
    "epochs = 4 #n_iters / (len(train_dataset) / batch_size)\n",
    "i = 0\n",
    "for epoch in range(int(epochs)):\n",
    "    epoch_losses = []\n",
    "    for it in range(N):\n",
    "        optimizer.zero_grad()\n",
    "        images, labels = get_minibatch(train_dataset, batch_size, it)    \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels) # NEXT STEP: replace with an evaluate call!!!\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.detach().item())\n",
    "        i += 1\n",
    "    # end of epoch: calculate Accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        outputs = model(images)\n",
    "        # TO DO convert these outputs to probs, to get more accurate Accuracy metric\n",
    "        # and to support historical averaging (e.g. q(n) = p(n) * 2/(n+1) + q(n-1) * (n-1)/(n+1))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
    "        correct+= (predicted == labels).sum()\n",
    "    accuracy = 100 * float(correct)/total\n",
    "    print(\"Iteration: {}. Loss: {:1f}. Accuracy: {}.\".format(i, torch.tensor(epoch_losses).mean(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "CPU times: user 156 ms, sys: 11.9 ms, total: 168 ms\n",
      "Wall time: 168 ms\n",
      "CPU times: user 1e+03 ns, sys: 3 µs, total: 4 µs\n",
      "Wall time: 4.77 µs\n",
      "tensor(-227.8024, grad_fn=<NegBackward>)\n",
      "torch.Size([10, 784]) torch.Size([10, 784])\n",
      "torch.Size([10]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "from HINTS_fn import *\n",
    "# PyTorch test function for HINTS\n",
    "# proposals are Langevin/HMC and must create a new deep copy of the model\n",
    "# HINTS uses the ID() of the model object for tracking/caching purposes\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "class TorchMNIST(UserFn):\n",
    "        def __init__(self, additive = True):\n",
    "            self.batch_size = 100 # 60000 dataset size (so will not see many GPU benefits)\n",
    "            self.input_dim = 784\n",
    "            self.output_dim = 10\n",
    "            self.train_dataset = dsets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "            self.N = self.train_dataset.__len__()//self.batch_size # num scenarios\n",
    "            self.lr = 0.001 # TO DO pass this parameter\n",
    "            self.additive = additive # used by HINTS\n",
    "            print(self.N)\n",
    "            self.criterion = torch.nn.CrossEntropyLoss(reduction = 'sum')\n",
    "            super().__init__(None)\n",
    "        #\n",
    "        def sample_initial_state(self):\n",
    "            model = LogisticRegression(self.input_dim, self.output_dim).to(device)\n",
    "            return(model)\n",
    "        #    \n",
    "        @lru_cache(maxsize = 100000)\n",
    "        def evaluate(self, state, term_index, gradient = False):\n",
    "            self.counter += 1\n",
    "            if gradient:\n",
    "                f = -self.minibatch_loss(state, term_index)\n",
    "                f.backward()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    f = -self.minibatch_loss(state, term_index)\n",
    "            return(f)\n",
    "        #\n",
    "        # this can be called with or without torch.no_grad\n",
    "        def minibatch_loss(self, model, term_index):\n",
    "            images, labels = get_minibatch(self.train_dataset, self.batch_size, term_index)  \n",
    "            outputs = model(images.to(device))\n",
    "            loss = self.criterion(outputs.cpu(), labels) # NEXT STEP: replace with an evaluate call!!!\n",
    "            return(loss)\n",
    "\n",
    "    \n",
    "MH = TorchMNIST()\n",
    "state0 = MH.sample_initial_state()\n",
    "%time v = MH.evaluate(state0, 6, True)\n",
    "%time v = MH.evaluate(state0, 6, True)\n",
    "print(v)\n",
    "for f in state0.parameters():\n",
    "    print(f.shape, f.grad.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Langevin \n",
    "#### this treatment assumes we are trying to minimise ... I should reverse sign of f\n",
    "p ~ N(0,1)\n",
    "p1 = p - eps/2 g(x)\n",
    "x' = x + p1\n",
    "p' = p1 - eps/2 g(x') = p - eps/2 (g(x') + g(x)) = p - eps gbar\n",
    "\n",
    "accept if u < exp(-dH)\n",
    "where dH = f(x') - f(x) + p'^2/2 - p^2/2\n",
    "\n",
    "second term is Hastings correction:\n",
    "p^2 - 2 p eps gbar + eps^2 gbar^2 - p^2 = eps gbar (eps gbar - 2p)\n",
    "\n",
    "in terms of p1: \n",
    "p1^2 - eps p1 g(x') + (eps/2)^2 g(x')^2 - p1^2 - eps p1 g(x) - (eps/2)^2 g(x)^2\n",
    "\n",
    "Alternative would be to include p as part of state (which we'll need to do for HMC anyway)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 600. Loss: 53.359703. Accuracy: 90.14.\n",
      "Iteration: 1200. Loss: 35.966660. Accuracy: 91.01.\n",
      "Iteration: 1800. Loss: 33.157066. Accuracy: 91.3.\n",
      "Iteration: 2400. Loss: 31.687439. Accuracy: 91.56.\n",
      "CPU times: user 23.6 s, sys: 13.7 s, total: 37.3 s\n",
      "Wall time: 20.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# MINIBATCH SGD EXAMPLE (no HINTS yet)\n",
    "epochs = 4\n",
    "lr_rate = 0.001\n",
    "model = MH.sample_initial_state()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = lr_rate)\n",
    "i = 0\n",
    "for epoch in range(int(epochs)):\n",
    "    epoch_losses = []\n",
    "    for it in range(MH.N):\n",
    "        optimizer.zero_grad()\n",
    "        loss = MH.minibatch_loss(model, it).cpu()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.detach().item())\n",
    "        i += 1\n",
    "    # end of epoch: calculate Accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        outputs = model(images.to(device)).cpu()\n",
    "        # TO DO convert these outputs to probs, to get more accurate Accuracy metric\n",
    "        # and to support historical averaging (e.g. q(n) = p(n) * 2/(n+1) + q(n-1) * (n-1)/(n+1))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
    "        correct+= (predicted == labels).sum()\n",
    "    accuracy = 100 * float(correct)/total\n",
    "    print(\"Iteration: {}. Loss: {:1f}. Accuracy: {}.\".format(i, torch.tensor(epoch_losses).mean(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU slightly reduces minibatch wall clock time (100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HINTS import *\n",
    "\n",
    "class HINTS_HMC(HINTS):\n",
    "    # TO DO move langevin out?\n",
    "    def __init__(self, args, fn, noise_sd = 0.01):\n",
    "        super().__init__(args, fn)\n",
    "        self.epsilon = args.epsilon\n",
    "        self.noise_sd = noise_sd\n",
    "    #\n",
    "    # HMC version ... correction depends on knowing gradient at proposed state\n",
    "    def primitive_move(self, model, index = 0, always_accept = False):\n",
    "        scenarios = self.scenarios(0, index)\n",
    "        #print(\"primitive\", index, len(scenarios))\n",
    "        v = self.fn(model, scenarios, True) # puts a gradient into state as a side effect for HMC\n",
    "        current = self.fn.sample_initial_state() # empty model\n",
    "        # do this with no grad...\n",
    "        correction = 0.0\n",
    "        for f, f_prime in zip(model.parameters(), current.parameters()):\n",
    "            # f.grad.data is the right shape to store momentum temporarily\n",
    "            ###TO DO how big is f.grad.data compared with unit noise\n",
    "            f.grad.data = self.noise_sd * torch.randn(f.shape).to(device) + self.epsilon * 0.5 * f.grad.data # TO DO check gradient convention\n",
    "            f_prime.data = f.data + 0.5 * self.epsilon * f.grad.data # add momentum\n",
    "            correction -= 0.5 * (f.grad.data * f.grad.data).sum() # Kinetic energy term of -H\n",
    "        # compute the value of the new state and its gradient\n",
    "        v_prime = self.fn(current, scenarios, True) # need gdt again\n",
    "        # store the new momentum in the grad entries of the candidate model\n",
    "        for f, f_prime in zip(model.parameters(), current.parameters()):\n",
    "            p_prime = f.grad.data + self.epsilon * 0.5 * f_prime.grad.data # TO DO check gradient convention\n",
    "            correction += 0.5 * (p_prime * p_prime).sum() # kinetic energy term of H_new\n",
    "            f.grad = None # must not reuse (unless we do more leapfrog steps)\n",
    "            f_prime.grad = None\n",
    "        \n",
    "        # standard MHR / HINTS acceptance\n",
    "        vdiff = (v_prime - v)/self.Ts[0] # PE change ... these are cached evaluations, no side effects\n",
    "        #print(v_prime, v, self.Ts[0], correction)\n",
    "        #\n",
    "        #correction = 0 # TEMPOARY OVERRDE - SGD\n",
    "        #\n",
    "        #\n",
    "        accept = True if always_accept else self.metropolis_accept(vdiff - correction)\n",
    "        (self.acceptances if accept else self.rejections)[0] += 1\n",
    "        return((current, vdiff) if accept else (model, 0.0))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "{'levels': 1, 'design': array([  1, 600]), 'additive': True, 'T': 1.0, 'dT': 0.0, 'epsilon': 0.02}\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_known_args()[0] # defaults\n",
    "\n",
    "# checked regular MCMC is accurate (1 level, lbf = 6)\n",
    "if False:\n",
    "    args.levels = 6\n",
    "    log_branch_factor = 1\n",
    "    N_0 = 1\n",
    "    #args.design = np.array([N_0] + [2 ** log_branch_factor for l in range(args.levels)])\n",
    "    args.design = np.array([1,2,3,2,5,2,5])\n",
    "    NUM_SCENARIOS = args.design.prod()\n",
    "    aa = False\n",
    "    #NUM_SCENARIOS = N_0 * 2 ** (args.levels * log_branch_factor) # TO DO get from HINTS\n",
    "elif True: # test Langevin MCMC first (or set noise_sd to small value for SGD)\n",
    "    args.levels = 1 \n",
    "    NUM_SCENARIOS = 600 # 600 for naive mcmc, 1 for SGD\n",
    "    args.design = np.array([1,NUM_SCENARIOS]) # ensure whole dataset is covered\n",
    "    aa = True # use always_accept flag to get minibatch Langevin (so we actually only have  a 1 level architecture)\n",
    "else: # full Langevin/gradient descent\n",
    "    args.levels = 0 \n",
    "    NUM_SCENARIOS = 600 # 600 for naive mcmc, 1 for SGD\n",
    "    args.design = np.array([NUM_SCENARIOS]) # ensure whole dataset is covered\n",
    "    aa = True # always accept only applies to non-primitive moves SO THIS WILL NOT WORK!!\n",
    "    \n",
    "    \n",
    "print(NUM_SCENARIOS)\n",
    "\n",
    "# design now has levels + 1 entriess\n",
    "\n",
    "# additive log probability is more natural from a Bayesian perspective but both are valid\n",
    "\n",
    "args.additive = True # effectively selects a different temperature structure when False (= average or expectation)\n",
    "\n",
    "args.T = 1.0 #top level\n",
    "args.dT = 0.0 if args.additive else 0.5 # temperature increment by level (mainly for optimisation or averaging structure)\n",
    "args.epsilon = 0.02 # for HMC\n",
    "print(args.__dict__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "RESET\n",
      "1\n",
      "[  1 600]\n",
      "600\n",
      "[1. 1.]\n",
      "LogisticRegression(\n",
      "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "g = TorchMNIST(args.additive)\n",
    "hmc = HINTS_HMC(args, g, noise_sd = 1e-3) # noise sd crucial for acceptance rate (check maths for sd not equal to 1\n",
    "state  = g.sample_initial_state()\n",
    "print(state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Iteration: 1. Loss: 82.80477142333984. Accuracy: 83.52. [411   1] [189   0]\n",
      "1\n",
      "Iteration: 2. Loss: 68.9365463256836. Accuracy: 86.54. [894   2] [306   0]\n",
      "2\n",
      "Iteration: 3. Loss: 53.33583450317383. Accuracy: 87.78. [1415    3] [385   0]\n",
      "3\n",
      "Iteration: 4. Loss: 31.505752563476562. Accuracy: 88.39. [1937    4] [463   0]\n",
      "4\n",
      "Iteration: 5. Loss: 44.37473678588867. Accuracy: 88.76. [2487    5] [513   0]\n",
      "5\n",
      "Iteration: 6. Loss: 50.96900177001953. Accuracy: 89.28. [3025    6] [575   0]\n",
      "6\n",
      "Iteration: 7. Loss: 57.9368896484375. Accuracy: 89.53. [3568    7] [632   0]\n",
      "7\n",
      "Iteration: 8. Loss: 34.14878845214844. Accuracy: 89.72. [4125    8] [675   0]\n",
      "8\n",
      "Iteration: 9. Loss: 54.5328369140625. Accuracy: 90.01. [4665    9] [735   0]\n",
      "9\n",
      "Iteration: 10. Loss: 43.673362731933594. Accuracy: 90.14. [5215   10] [785   0]\n",
      "10\n",
      "Iteration: 11. Loss: 59.189971923828125. Accuracy: 90.34. [5771   11] [829   0]\n",
      "11\n",
      "Iteration: 12. Loss: 43.510948181152344. Accuracy: 90.42. [6322   12] [878   0]\n",
      "12\n",
      "Iteration: 13. Loss: 41.03447723388672. Accuracy: 90.54. [6876   13] [924   0]\n",
      "13\n",
      "Iteration: 14. Loss: 49.55488204956055. Accuracy: 90.54. [7425   14] [975   0]\n",
      "14\n",
      "Iteration: 15. Loss: 26.017269134521484. Accuracy: 90.62. [7984   15] [1016    0]\n",
      "15\n",
      "Iteration: 16. Loss: 39.81026077270508. Accuracy: 90.67. [8541   16] [1059    0]\n",
      "16\n",
      "Iteration: 17. Loss: 29.794485092163086. Accuracy: 90.69. [9092   17] [1108    0]\n",
      "17\n",
      "Iteration: 18. Loss: 21.623920440673828. Accuracy: 90.9. [9648   18] [1152    0]\n",
      "18\n",
      "Iteration: 19. Loss: 25.401355743408203. Accuracy: 90.92. [10191    19] [1209    0]\n",
      "19\n",
      "Iteration: 20. Loss: 20.83535385131836. Accuracy: 91.0. [10747    20] [1253    0]\n",
      "20\n",
      "Iteration: 21. Loss: 39.97578430175781. Accuracy: 91.04. [11310    21] [1290    0]\n",
      "21\n",
      "Iteration: 22. Loss: 20.434926986694336. Accuracy: 91.13. [11865    22] [1335    0]\n",
      "22\n",
      "Iteration: 23. Loss: 23.76821517944336. Accuracy: 91.19. [12418    23] [1382    0]\n",
      "23\n",
      "Iteration: 24. Loss: 21.592327117919922. Accuracy: 91.23. [12977    24] [1423    0]\n",
      "24\n",
      "Iteration: 25. Loss: 30.5335693359375. Accuracy: 91.35. [13525    25] [1475    0]\n",
      "25\n",
      "Iteration: 26. Loss: 16.78483772277832. Accuracy: 91.3. [14079    26] [1521    0]\n",
      "26\n",
      "Iteration: 27. Loss: 32.78132247924805. Accuracy: 91.41. [14642    27] [1558    0]\n",
      "27\n",
      "Iteration: 28. Loss: 30.52320098876953. Accuracy: 91.43. [15206    28] [1594    0]\n",
      "28\n",
      "Iteration: 29. Loss: 26.92966079711914. Accuracy: 91.53. [15771    29] [1629    0]\n",
      "29\n",
      "Iteration: 30. Loss: 28.164609909057617. Accuracy: 91.58. [16335    30] [1665    0]\n",
      "30\n",
      "Iteration: 31. Loss: 40.313438415527344. Accuracy: 91.49. [16896    31] [1704    0]\n",
      "31\n",
      "Iteration: 32. Loss: 20.156185150146484. Accuracy: 91.59. [17453    32] [1747    0]\n",
      "32\n",
      "Iteration: 33. Loss: 33.884498596191406. Accuracy: 91.61. [18006    33] [1794    0]\n",
      "33\n",
      "Iteration: 34. Loss: 27.12082862854004. Accuracy: 91.58. [18570    34] [1830    0]\n",
      "34\n",
      "Iteration: 35. Loss: 18.060951232910156. Accuracy: 91.69. [19130    35] [1870    0]\n",
      "35\n",
      "Iteration: 36. Loss: 29.790863037109375. Accuracy: 91.64. [19676    36] [1924    0]\n",
      "36\n",
      "Iteration: 37. Loss: 34.872440338134766. Accuracy: 91.66. [20241    37] [1959    0]\n",
      "37\n",
      "Iteration: 38. Loss: 37.79018783569336. Accuracy: 91.61. [20800    38] [2000    0]\n",
      "38\n",
      "Iteration: 39. Loss: 18.586246490478516. Accuracy: 91.67. [21368    39] [2032    0]\n",
      "39\n",
      "Iteration: 40. Loss: 16.028507232666016. Accuracy: 91.71. [21925    40] [2075    0]\n",
      "40\n",
      "Iteration: 41. Loss: 31.619125366210938. Accuracy: 91.75. [22490    41] [2110    0]\n",
      "41\n",
      "Iteration: 42. Loss: 41.044715881347656. Accuracy: 91.68. [23055    42] [2145    0]\n",
      "42\n",
      "Iteration: 43. Loss: 25.191631317138672. Accuracy: 91.81. [23609    43] [2191    0]\n",
      "43\n",
      "Iteration: 44. Loss: 24.64937973022461. Accuracy: 91.74. [24168    44] [2232    0]\n",
      "44\n",
      "Iteration: 45. Loss: 19.85930061340332. Accuracy: 91.71. [24726    45] [2274    0]\n",
      "45\n",
      "Iteration: 46. Loss: 20.08673858642578. Accuracy: 91.79. [25294    46] [2306    0]\n",
      "46\n",
      "Iteration: 47. Loss: 32.33281707763672. Accuracy: 91.79. [25852    47] [2348    0]\n",
      "47\n",
      "Iteration: 48. Loss: 24.391141891479492. Accuracy: 91.87. [26407    48] [2393    0]\n",
      "48\n",
      "Iteration: 49. Loss: 26.523670196533203. Accuracy: 91.88. [26975    49] [2425    0]\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "for t in range(1000):\n",
    "    hmc.shuffle()\n",
    "    print(t)\n",
    "    g.evaluate.cache_clear() # risk of revisiting same state and scenario after reject, and gradient not being available\n",
    "    state, correction = hmc.hints(state, args.levels, always_accept = aa) # e.g. dbg = (t==0)\n",
    "    # diagnostic histogram\n",
    "    # show progress\n",
    "    if True:\n",
    "        with torch.no_grad():\n",
    "            loss = MH.minibatch_loss(state, t % NUM_SCENARIOS)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = Variable(images.view(-1, 28*28))\n",
    "            outputs = state(images.to(device))\n",
    "            # TO DO convert these outputs to probs, to get more accurate Accuracy metric\n",
    "            # and to support historical averaging (e.g. q(n) = p(n) * 2/(n+1) + q(n-1) * (n-1)/(n+1))\n",
    "            _, predicted = torch.max(outputs.cpu().data, 1)\n",
    "            total += labels.size(0)\n",
    "            # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
    "            correct+= (predicted == labels).sum()\n",
    "        accuracy = 100 * float(correct)/total\n",
    "        print(\"Iteration: {}. Loss: {}. Accuracy: {}.\"\\\n",
    "              .format(t+1, loss, accuracy), hmc.acceptances, hmc.rejections)\n",
    "\n",
    "#TO DO skip accept/reject at top level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9540 6752 miss% = 70\n"
     ]
    }
   ],
   "source": [
    "print(g.total_counter, g.counter, \"miss% = \" + str(int((100.0 * g.counter)/ g.total_counter))) # check cache ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks to be better than MCMC [but for this dataset, no compelling case for MCMC methods]\n",
    "# TO DO higher branch factor (4+) more efficient\n",
    "# HOW MUCH NOISE AT PRIMITIVE LEVEL? - zero case for SGD\n",
    "# aim for bigger moves at primitive level (or HMC chain?)\n",
    "# GPU - DONE\n",
    "# Bayesian accuracy measure thru decaying average (triangle distrib)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
