{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gradient based (Langevin or Hamiltonian) HINTS\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from HINTS_fn import *\n",
    "from HINTS_torch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "577d337fccac4a36b9c2ccc8ea119fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f73b1ebb0e2c444aa979280c83b9cd4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276caa22e244466cb3548c01ed4b49f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19ae5b543344766ae18b577da1366da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first test PyTorch\n",
    "train_dataset = dsets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = dsets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
    "batch_size = 100\n",
    "#train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example PyTorch model\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x) # pytorch Crossentropy loss takes scores, not logits as inputs\n",
    "        #outputs = F.softmax(self.linear(x), dim = -1) # why better without F.softmax\n",
    "        return outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# was not worth caching this\n",
    "def get_minibatch(dataset, n, i, input_dim = 784):\n",
    "    lst1, lst2 = zip(*[dataset[ii] for ii in range(i * n, (i+1) * n)])\n",
    "    xs = torch.stack(lst1).view(-1, input_dim)\n",
    "    ys = torch.tensor(lst2)\n",
    "    return(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "Iteration: 600. Loss: 53.563038. Accuracy: 90.16.\n",
      "Iteration: 1200. Loss: 35.924446. Accuracy: 90.94.\n",
      "Iteration: 1800. Loss: 33.135147. Accuracy: 91.37.\n",
      "Iteration: 2400. Loss: 31.675495. Accuracy: 91.61.\n",
      "Wall time: 47.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_dim = 784\n",
    "output_dim = 10\n",
    "\n",
    "# Note we will use a Bayesian model average of (some distribution over) history of sampled class probs in the sampling case\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction = 'sum') # computes softmax and then the cross entropy\n",
    "model = LogisticRegression(input_dim, output_dim)\n",
    "lr_rate = 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate) # SToch Gdt Descent\n",
    "N = train_dataset.__len__()//batch_size # TO DO: get N from dataset\n",
    "print(N)\n",
    "epochs = 4 #n_iters / (len(train_dataset) / batch_size)\n",
    "i = 0\n",
    "for epoch in range(int(epochs)):\n",
    "    epoch_losses = []\n",
    "    for it in range(N):\n",
    "        optimizer.zero_grad()\n",
    "        images, labels = get_minibatch(train_dataset, batch_size, it)    \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels) # NEXT STEP: replace with an evaluate call!!!\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.detach().item())\n",
    "        i += 1\n",
    "    # end of epoch: calculate Accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        outputs = model(images)\n",
    "        # TO DO convert these outputs to probs, to get more accurate Accuracy metric\n",
    "        # and to support historical averaging (e.g. q(n) = p(n) * 2/(n+1) + q(n-1) * (n-1)/(n+1))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
    "        correct+= (predicted == labels).sum()\n",
    "    accuracy = 100 * float(correct)/total\n",
    "    print(\"Iteration: {}. Loss: {:1f}. Accuracy: {}.\".format(i, torch.tensor(epoch_losses).mean(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the user function for HINTS\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # GPU slightly reduces minibatch wall clock time (100)\n",
    "\n",
    "class TorchMNIST(UserFn):\n",
    "        def __init__(self, additive = True):\n",
    "            self.batch_size = 100 # 60000 dataset size (so will not see many GPU benefits)\n",
    "            self.input_dim = 784\n",
    "            self.output_dim = 10\n",
    "            self.train_dataset = dsets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "            self.N = self.train_dataset.__len__()//self.batch_size # num scenarios\n",
    "            self.lr = 0.001 # TO DO pass this parameter\n",
    "            self.additive = additive # used by HINTS\n",
    "            print(self.N)\n",
    "            self.criterion = torch.nn.CrossEntropyLoss(reduction = 'sum')\n",
    "            #self.calls = [] # for cache stats\n",
    "            super().__init__(None)\n",
    "        #\n",
    "        def sample_initial_state(self):\n",
    "            model = LogisticRegression(self.input_dim, self.output_dim).to(device)\n",
    "            return(model)\n",
    "        #    \n",
    "        @lru_cache(maxsize = 1000000)\n",
    "        def evaluate(self, state, term_index, gradient = False):\n",
    "            #self.calls.append((hash(self), hash(state), hash(term_index), hash(gradient)))\n",
    "            self.counter += 1\n",
    "            if gradient:\n",
    "                f = -self.minibatch_loss(state, term_index)\n",
    "                f.backward()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    f = -self.minibatch_loss(state, term_index)\n",
    "            return(f)\n",
    "        #\n",
    "        # this can be called with or without torch.no_grad\n",
    "        def minibatch_loss(self, model, term_index):\n",
    "            images, labels = get_minibatch(self.train_dataset, self.batch_size, term_index)  \n",
    "            outputs = model(images.to(device))\n",
    "            loss = self.criterion(outputs.cpu(), labels) # NEXT STEP: replace with an evaluate call!!!\n",
    "            return(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "Wall time: 27 ms\n",
      "Wall time: 0 ns\n",
      "tensor(-231.8298, grad_fn=<NegBackward0>)\n",
      "torch.Size([10, 784]) torch.Size([10, 784])\n",
      "torch.Size([10]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# test it   \n",
    "MH = TorchMNIST()\n",
    "state0 = MH.sample_initial_state()\n",
    "state1 = state0\n",
    "%time v = MH.evaluate(state0, 6, True)\n",
    "%time v = MH.evaluate(state1, 6, True) # cached: should be faster\n",
    "print(v)\n",
    "for f in state0.parameters():\n",
    "    print(f.shape, f.grad.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Langevin \n",
    "#### this treatment assumes we are trying to minimise ... \n",
    "p ~ N(0,1)\n",
    "p1 = p - eps/2 g(x)\n",
    "x' = x + p1\n",
    "p' = p1 - eps/2 g(x') = p - eps/2 (g(x') + g(x)) = p - eps gbar\n",
    "\n",
    "accept if u < exp(-dH)\n",
    "where dH = f(x') - f(x) + p'^2/2 - p^2/2\n",
    "\n",
    "second term is Hastings correction:\n",
    "p^2 - 2 p eps gbar + eps^2 gbar^2 - p^2 = eps gbar (eps gbar - 2p)\n",
    "\n",
    "in terms of p1: \n",
    "p1^2 - eps p1 g(x') + (eps/2)^2 g(x')^2 - p1^2 - eps p1 g(x) - (eps/2)^2 g(x)^2\n",
    "\n",
    "Alternative would be to include p as part of state (which we'll need to do for HMC anyway)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 600. Loss: 53.401009. Accuracy: 90.24.\n",
      "Iteration: 1200. Loss: 35.948322. Accuracy: 90.98.\n",
      "Iteration: 1800. Loss: 33.153053. Accuracy: 91.41.\n",
      "Iteration: 2400. Loss: 31.689238. Accuracy: 91.59.\n",
      "Wall time: 47.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# MINIBATCH SGD EXAMPLE (no HINTS yet)\n",
    "epochs = 4\n",
    "lr_rate = 0.001\n",
    "model = MH.sample_initial_state()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = lr_rate)\n",
    "i = 0\n",
    "for epoch in range(int(epochs)):\n",
    "    epoch_losses = []\n",
    "    for it in range(MH.N):\n",
    "        optimizer.zero_grad()\n",
    "        loss = MH.minibatch_loss(model, it).cpu()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.detach().item())\n",
    "        i += 1\n",
    "    # end of epoch: calculate Accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        outputs = model(images.to(device)).cpu()\n",
    "        # TO DO convert these outputs to probs, to get more accurate Accuracy metric\n",
    "        # and to support historical averaging (e.g. q(n) = p(n) * 2/(n+1) + q(n-1) * (n-1)/(n+1))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
    "        correct+= (predicted == labels).sum()\n",
    "    accuracy = 100 * float(correct)/total\n",
    "    print(\"Iteration: {}. Loss: {:1f}. Accuracy: {}.\".format(i, torch.tensor(epoch_losses).mean(), accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "{'levels': 6, 'design': array([1, 2, 3, 2, 5, 2, 5]), 'additive': True, 'T': 1.0, 'dT': 0.0, 'epsilon': 0.02}\n",
      "600\n",
      "RESET\n",
      "6\n",
      "[  1   2   6  12  60 120 600]\n",
      "600\n",
      "[1. 1. 1. 1. 1. 1. 1.]\n",
      "LogisticRegression(\n",
      "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_known_args()[0] # defaults\n",
    "\n",
    "\n",
    "if True: # set to True for HINTS\n",
    "    args.levels = 6\n",
    "    log_branch_factor = 1\n",
    "    N_0 = 1\n",
    "    args.design = np.array([1,2,3,2,5,2,5])\n",
    "    NUM_SCENARIOS = args.design.prod()\n",
    "    aa = False\n",
    "    sd = 0.01\n",
    "elif True: # test Langevin MCMC first (or set sd to small value for SGD)\n",
    "    args.levels = 1 \n",
    "    NUM_SCENARIOS = 600 # Langevin / SGD\n",
    "    args.design = np.array([1,NUM_SCENARIOS]) # ensure whole dataset is covered\n",
    "    aa = True # use always_accept flag to get minibatch Langevin (so we actually only have  a 1 level architecture) - TOP LEVEL ALWAYS ACCEPT\n",
    "    sd = 0.001\n",
    "else: # FULL DATASET (not minibatch) Langevin/gradient descent\n",
    "    args.levels = 0 \n",
    "    NUM_SCENARIOS = 600 # 600 for naive mcmc, 1 for SGD\n",
    "    args.design = np.array([NUM_SCENARIOS]) # ensure whole dataset is covered\n",
    "    aa = True # always accept only applies to TOP LEVEL\n",
    "    sd = 0.001\n",
    "    \n",
    "print(NUM_SCENARIOS)\n",
    "\n",
    "# design now has levels + 1 entries: so we can give a minibatch size in design[0]\n",
    "# additive log probability is more natural from a Bayesian perspective but both are valid\n",
    "\n",
    "args.additive = True # effectively selects a different temperature structure when False (= average or expectation)\n",
    "\n",
    "args.T = 1.0 #top level\n",
    "args.dT = 0.0 if args.additive else 0.5 # temperature increment by level (mainly for optimisation or averaging structure)\n",
    "args.epsilon = 0.02 # for HMC\n",
    "print(args.__dict__)\n",
    "\n",
    "\n",
    "g = TorchMNIST(args.additive)\n",
    "hmc = HINTS_HMC(args, g, noise_sd = sd) # noise sd crucial for acceptance rate (check maths for sd not equal to 1\n",
    "state  = g.sample_initial_state()\n",
    "print(state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1716904181.2081645\n",
      "0\n",
      "Iteration: 1. Train time:102.31. Test time:1.67. Noisy loss: 85.35. Accuracy: 83.44. [389 300 100  50  10   5   1] [211   0   0   0   0   0   0]\n",
      "1\n",
      "Iteration: 2. Train time:104.16. Test time:1.66. Noisy loss: 67.72. Accuracy: 86.5. [885 599 199 100  20  10   2] [315   1   1   0   0   0   0]\n",
      "2\n",
      "Iteration: 3. Train time:104.99. Test time:1.67. Noisy loss: 54.3. Accuracy: 87.55. [1407  892  299  148   30   15    3] [393   8   1   2   0   0   0]\n",
      "3\n",
      "Iteration: 4. Train time:105.23. Test time:1.67. Noisy loss: 31.78. Accuracy: 88.21. [1954 1185  399  198   40   20    4] [446  15   1   2   0   0   0]\n",
      "4\n",
      "Iteration: 5. Train time:104.29. Test time:1.66. Noisy loss: 44.66. Accuracy: 88.78. [2499 1478  498  246   50   25    5] [501  22   2   4   0   0   0]\n",
      "5\n",
      "Iteration: 6. Train time:103.71. Test time:1.66. Noisy loss: 52.6. Accuracy: 89.08. [3035 1769  594  295   60   30    6] [565  31   6   5   0   0   0]\n",
      "6\n",
      "Iteration: 7. Train time:104.05. Test time:1.67. Noisy loss: 58.94. Accuracy: 89.38. [3579 2066  692  341   70   35    7] [621  34   8   9   0   0   0]\n",
      "7\n",
      "Iteration: 8. Train time:103.16. Test time:1.67. Noisy loss: 34.89. Accuracy: 89.67. [4126 2360  790  384   80   40    8] [674  40  10  16   0   0   0]\n",
      "8\n",
      "Iteration: 9. Train time:107.86. Test time:1.75. Noisy loss: 55.62. Accuracy: 89.85. [4673 2654  884  429   90   45    9] [727  46  16  21   0   0   0]\n",
      "9\n",
      "Iteration: 10. Train time:103.46. Test time:1.7. Noisy loss: 44.36. Accuracy: 90.1. [5229 2946  980  474  100   50   10] [771  54  20  26   0   0   0]\n",
      "10\n",
      "Iteration: 11. Train time:103.64. Test time:1.67. Noisy loss: 60.64. Accuracy: 90.16. [5783 3237 1073  522  110   55   11] [817  63  27  28   0   0   0]\n",
      "11\n",
      "Iteration: 12. Train time:102.46. Test time:1.74. Noisy loss: 45.22. Accuracy: 90.32. [6327 3531 1168  569  118   60   12] [873  69  32  31   2   0   0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11785"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 0\n",
    "time0 = time.time()\n",
    "print(time0)\n",
    "now = time0\n",
    "then = now\n",
    "while (now < (time0 + 1200.0)):\n",
    "    hmc.shuffle()\n",
    "    print(t)\n",
    "    g.evaluate.cache_clear() # risk of revisiting same state and scenario after reject, and gradient not being available\n",
    "    state, correction = hmc.hints(state, args.levels, always_accept = aa) # e.g. dbg = (t==0)\n",
    "    # diagnostic histogram\n",
    "    # show progress\n",
    "    prev_then = then\n",
    "    then = time.time()\n",
    "    t+=1\n",
    "    if True:\n",
    "        with torch.no_grad():\n",
    "            loss = MH.minibatch_loss(state, (t-1) % NUM_SCENARIOS)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = Variable(images.view(-1, 28*28))\n",
    "            outputs = state(images.to(device))\n",
    "            # TO DO convert these outputs to probs, to get more accurate Accuracy metric\n",
    "            # and to support historical averaging (e.g. q(n) = p(n) * 2/(n+1) + q(n-1) * (n-1)/(n+1))\n",
    "            _, predicted = torch.max(outputs.cpu().data, 1)\n",
    "            total += labels.size(0)\n",
    "            # for gpu, bring the predicted and labels back to cpu fro python operations to work\n",
    "            correct+= (predicted == labels).sum()\n",
    "        accuracy = 100 * float(correct)/total\n",
    "        prev_now = now\n",
    "        now = time.time()\n",
    "        print(\"Iteration: {}. Train time:{}. Test time:{}. Noisy loss: {}. Accuracy: {}.\"\\\n",
    "              .format(t, round(then - prev_now, 2), round(now - then, 2), round(loss.item(),2), accuracy), hmc.acceptances, hmc.rejections)\n",
    "    else:\n",
    "        now = time.time()\n",
    "        print(\"Iteration: {}. Time taken:{}.\"\\\n",
    "              .format(t, round(then - prev_then, 2), hmc.acceptances, hmc.rejections))\n",
    "\n",
    "# print time per evaluation\n",
    "hmc.acceptances.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800 12394 miss% = 73\n"
     ]
    }
   ],
   "source": [
    "print(g.total_counter, g.counter, \"miss% = \" + str(int((100.0 * g.counter)/ g.total_counter))) # check cache ratio\n",
    "#(len(set(g.calls)), len(g.calls)) # check potential gain from cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CacheInfo(hits=2136, misses=6264, maxsize=1000000, currsize=6264)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.evaluate.cache_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks to be better than MCMC [but for this dataset, no compelling case for MCMC methods]\n",
    "# HOW MUCH NOISE AT PRIMITIVE LEVEL? - zero case for SGD\n",
    "# aim for bigger moves at primitive level (or HMC chain?)\n",
    "# GPU - DONE\n",
    "# Bayesian accuracy measure thru decaying average (triangle distrib)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
